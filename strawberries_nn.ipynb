{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15f8bd5-4076-4e35-bac7-9f74d489515d",
   "metadata": {},
   "source": [
    "# Time Series Analysis & Image Classification of Strawberry Ripeness\n",
    "### Project Overview:\n",
    "-----------------\n",
    "This project focuses on building a deep learning pipeline to classify \n",
    "strawberry images into three categories: Occluded, Ripe, and Unripe. \n",
    "The dataset consists of 3,367 images with class imbalance.\n",
    "\n",
    "Key Steps:\n",
    "1. Data Loading: Load and inspect images and labels.\n",
    "2. Data Preprocessing:\n",
    "   - Remap labels to [0, 1, 2] to match model requirements.\n",
    "   - Train/Test split (80%/20%) with stratification to preserve class proportions.\n",
    "   - Resize images to uniform dimensions (e.g., 224x224 for MobileNetV2) and normalize pixel values to [0,1].\n",
    "3. Handling Class Imbalance:\n",
    "   - Compute class weights.\n",
    "   - Optionally, augment minority classes to balance the training data.\n",
    "4. Model Building:\n",
    "   - CNN model with convolutional, pooling, and dense layers.\n",
    "   - MobileNetV2 transfer learning model for comparison.\n",
    "5. Training:\n",
    "   - Use EarlyStopping to prevent overfitting.\n",
    "   - Train on original and balanced datasets.\n",
    "6. Evaluation:\n",
    "   - Compute training and test accuracy.\n",
    "   - Generate confusion matrices to analyze class-level performance.\n",
    "7. Conclusion:\n",
    "   - Compare model performances with/without balancing.\n",
    "   - Discuss accuracy vs class imbalance trade-offs.\n",
    "\n",
    "This notebook demonstrates practical approaches to deep learning-based \n",
    "image classification, handling class imbalance, and evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e42ffe-ff4f-4563-8998-a2790b276be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://pages.scinet.utoronto.ca/~ejspence/strawberries.npz\"\n",
    "urllib.request.urlretrieve(url, \"strawberries.npz\")\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741defa4-658d-47c2-b832-c9006931ff7b",
   "metadata": {},
   "source": [
    "- This step downloads the dataset file strawberries.npz from the provided URL and saves it locally. It ensures the data is available in the working directory for the remainder of the script. No data processing happens here; it only fetches the data file required for the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe14aec-51b2-4593-8118-646ccb82b280",
   "metadata": {},
   "source": [
    "## 1. Basic Script Skeleton + Imports + Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9ee0f8-c128-4b61-b3c4-82ab05e6ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Numerical and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import compute_class_weight, class_weight\n",
    "\n",
    "# Fix seeds for reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd036e-eb05-4eeb-b4b4-83518d3d897d",
   "metadata": {},
   "source": [
    "## 2. Loading & Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fbeecf-6c49-43b6-bac5-a167c4c57f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n",
      "X shape: (3367, 181, 131, 3)\n",
      "y shape: (3367,)\n",
      "X dtype: uint8\n",
      "y dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "  Class 1: 499 samples\n",
      "  Class 2: 462 samples\n",
      "  Class 3: 2406 samples\n"
     ]
    }
   ],
   "source": [
    "# === Load the .npz dataset ===\n",
    "data = np.load(\"strawberries.npz\")\n",
    "X = data[\"x\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "print(\"Data Loaded.\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X dtype:\", X.dtype)\n",
    "print(\"y dtype:\", y.dtype)\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"\\nClass Distribution:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4cde9-2e75-4400-a7f6-7909b80272a7",
   "metadata": {},
   "source": [
    "- The dataset contains 3367 color images with resolution 181×131 (3 channels). Labels are integers and the original label set uses values {1,2,3}.\n",
    "- The class distribution shows a strong class imbalance: class 3 (2406 samples) is the majority (~71%), while classes 1 and 2 are minority classes (499 and 462 samples respectively).\n",
    "- Noting this imbalance early is important because class frequency can bias the trained model toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90d511-75bf-4a48-a701-93d178af14a4",
   "metadata": {},
   "source": [
    "## 3. Fix labels + normalize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bbd5eb-98a7-4eca-bd13-de2c543df52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After remapping labels (now 0,1,2):\n",
      "  Class 0: 499 samples\n",
      "  Class 1: 462 samples\n",
      "  Class 2: 2406 samples\n"
     ]
    }
   ],
   "source": [
    "# === Remap labels from {1,2,3} to {0,1,2} ===\n",
    "# This is important because sparse_categorical_crossentropy expects labels in [0, num_classes-1]\n",
    "y = y - 1  # 1->0, 2->1, 3->2\n",
    "\n",
    "unique2, counts2 = np.unique(y, return_counts=True)\n",
    "print(\"\\nAfter remapping labels (now 0,1,2):\")\n",
    "for u, c in zip(unique2, counts2):\n",
    "    print(f\"  Class {u}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3cafa-9723-404b-844e-541d41df7afb",
   "metadata": {},
   "source": [
    "- Kerasʼ sparse_categorical_crossentropy expects labels to start from 0. This step subtracts 1 from all labels so classes become {0,1,2}. The remapped counts confirm the earlier imbalance (Class 2 remains the large majority). This simple, necessary preprocessing step prevents indexing errors during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82466c04-a70f-4a2d-845a-56612c6126ac",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49a0dd8-5323-4b29-86d5-2b08457a65ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test Split Done.\n",
      "X_train: (2693, 181, 131, 3)\n",
      "X_test: (674, 181, 131, 3)\n",
      "y_train: (2693,)\n",
      "y_test: (674,)\n"
     ]
    }
   ],
   "source": [
    "# === Train/Test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.20,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test Split Done.\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc620a-67bf-4b25-8433-e20d871fe8a7",
   "metadata": {},
   "source": [
    "- The data were split into training (80%) and test (20%) sets with stratification to preserve class proportions. This ensures the test set is representative of the original distribution and prevents data leakage.\n",
    "- The split sizes (2693 train, 674 test) are appropriate for training a CNN while keeping a sufficiently large unseen test set for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ff513-adcf-436d-beb4-0e27f665141c",
   "metadata": {},
   "source": [
    "## 5. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15049bd0-2991-4b87-aa7c-442181c21bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data normalized to [0, 1].\n"
     ]
    }
   ],
   "source": [
    "# === Normalize images to [0,1] ===\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(\"\\nData normalized to [0, 1].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c839a-252c-4003-8ab6-0dc2df82d4e3",
   "metadata": {},
   "source": [
    "- Image pixel values (uint8 0-255) were converted to float32 and scaled to the [0,1] range. Normalization stabilizes training (improves gradient behavior) and is standard preprocessing for neural networks. This ensures numerical stability and faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1fc74-d4fe-43b7-ad2a-15afd3866170",
   "metadata": {},
   "source": [
    "## 6. Build the Standard CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75cc227b-2b8c-4334-aaec-618dc057713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">181</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">131</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45056</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,767,296</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m181\u001b[0m, \u001b[38;5;34m131\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45056\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m5,767,296\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m387\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,860,931</span> (22.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,860,931\u001b[0m (22.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,860,931</span> (22.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,860,931\u001b[0m (22.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Build the CNN model ===\n",
    "input_shape = X_train.shape[1:]  # (181, 131, 3)\n",
    "num_classes = 3  # 0: Occluded, 1: Ripe, 2: Unripe\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional Block 1\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Convolutional Block 2\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Convolutional Block 3\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten + Dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))  # helps reduce overfitting\n",
    "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(\"\\nModel summary:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5366a8e-ce0b-40dd-b6c8-a9b1c8d68b17",
   "metadata": {},
   "source": [
    "- A convolutional neural network (CNN) was constructed with three convolutional blocks (32 → 64 → 128 filters), followed by flattening, a 128-unit dense layer and a softmax output for three classes. The model has ~5.86M parameters.\n",
    "- The architecture is appropriate for image tasks because convolutional layers learn spatial features (edges, shapes, textures) that are crucial for distinguishing ripe/unripe/occluded strawberries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ebb31-be2b-406a-9b24-571bcdb938fc",
   "metadata": {},
   "source": [
    "## 6.1 Train the Model + Print Final Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811e7e3c-299e-487d-aef8-bdef00a73bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 793ms/step - accuracy: 0.7965 - loss: 0.5276 - val_accuracy: 0.8593 - val_loss: 0.3729\n",
      "Epoch 2/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 791ms/step - accuracy: 0.8514 - loss: 0.4064 - val_accuracy: 0.8630 - val_loss: 0.3571\n",
      "Epoch 3/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 789ms/step - accuracy: 0.8638 - loss: 0.3735 - val_accuracy: 0.8667 - val_loss: 0.3616\n",
      "Epoch 4/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 813ms/step - accuracy: 0.8675 - loss: 0.3751 - val_accuracy: 0.8556 - val_loss: 0.3720\n",
      "Epoch 5/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 798ms/step - accuracy: 0.8688 - loss: 0.3485 - val_accuracy: 0.8704 - val_loss: 0.3648\n",
      "Epoch 6/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 815ms/step - accuracy: 0.8816 - loss: 0.3251 - val_accuracy: 0.8741 - val_loss: 0.3399\n",
      "Epoch 7/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 788ms/step - accuracy: 0.8898 - loss: 0.2940 - val_accuracy: 0.8741 - val_loss: 0.3649\n",
      "Epoch 8/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 854ms/step - accuracy: 0.8906 - loss: 0.3008 - val_accuracy: 0.8667 - val_loss: 0.3650\n",
      "Epoch 9/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 774ms/step - accuracy: 0.9014 - loss: 0.2622 - val_accuracy: 0.8852 - val_loss: 0.3636\n",
      "Epoch 10/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 890ms/step - accuracy: 0.9096 - loss: 0.2470 - val_accuracy: 0.8852 - val_loss: 0.3917\n",
      "Epoch 11/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - accuracy: 0.9158 - loss: 0.2294 - val_accuracy: 0.8741 - val_loss: 0.3781\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Final training accuracy: 0.9158\n"
     ]
    }
   ],
   "source": [
    "# === Training ===\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,             # stop if val_loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,   # 10% of training data used for validation\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get final training accuracy from the history\n",
    "final_train_acc = history.history[\"accuracy\"][-1]\n",
    "print(f\"\\nFinal training accuracy: {final_train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe4e44-1359-4434-ae24-6ee1996af72a",
   "metadata": {},
   "source": [
    "- The model was trained with early stopping on validation loss (patience=5), which stopped training at epoch 11 and restored the best weights from epoch 6.\n",
    "- Final training accuracy (on the training portion used in fit) was **91.58%**.\n",
    "- Early stopping prevented excessive overfitting by halting training when validation loss stopped improving.\n",
    "- The validation accuracies shown during training demonstrate steady improvement and then stabilization, a sign of well-behaved training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6505c6-efa2-4406-91fb-c6fc878918ff",
   "metadata": {},
   "source": [
    "## 6.2 Evaluate on Test Data + Print Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebbcdb9-c81b-4ff8-a377-d62ccc6ffc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test data...\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195ms/step - accuracy: 0.8813 - loss: 0.3337\n",
      "\n",
      "Test accuracy: 0.8813\n"
     ]
    }
   ],
   "source": [
    "# === Evaluation on test data ===\n",
    "print(\"\\nEvaluating on test data...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ef534-8f20-4e43-88da-247baa5ed6c5",
   "metadata": {},
   "source": [
    "- When evaluated on the held-out test set (which was never used for training or augmentation), the model achieved **88.13% test accuracy** with a test loss of **0.3337**.\n",
    "- Test loss is an important supplement to accuracy: a low loss indicates the model's predicted probability distributions are well-calibrated and confident when correct.\n",
    "- The small gap between train (91.58%) and test (88.13%) suggests the model generalizes well without severe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "674eb29e-e95e-4603-8523-5e657bc629b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 32  17  51]\n",
      " [  5  83   4]\n",
      " [  2   1 479]]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Confusion Matrix\n",
    "# --------------------------------------------------\n",
    "# Predict probabilities\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities → class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc53d70e-0153-410a-a0f8-0ad67b975617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAHUCAYAAABIykBjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABO2UlEQVR4nO3deVxU1fsH8M+wL8LIIiAKirK4C+64gSmuuWTlWoKiLaBCoPglv4qmCZKpuaEpiluSlZhakmuoqSWmuYQb4g65gKDIPvf3hz/n2wgqgzNc4H7eveaVc+6ZM8/MoPPwnHPPlQmCIICIiIgkSUfsAIiIiEg8TASIiIgkjIkAERGRhDERICIikjAmAkRERBLGRICIiEjCmAgQERFJGBMBIiIiCWMiQEREJGFMBKhaOXPmDMaOHQsnJycYGRmhVq1aaNOmDaKjo5GZmanV5z516hS8vLwgl8shk8mwePFijT+HTCbDrFmzND7uq8TFxUEmk0Emk+HXX38tdVwQBDg7O0Mmk8Hb27tCz7FixQrExcWp9Zhff/31hTERkWboiR0AUXmtXr0aAQEBcHNzw9SpU9GsWTMUFRUhOTkZK1euxLFjx5CQkKC15x83bhxyc3MRHx8PCwsLNGzYUOPPcezYMdSvX1/j45aXmZkZYmNjS33ZJyUlITU1FWZmZhUee8WKFbC2toafn1+5H9OmTRscO3YMzZo1q/DzEtHLMRGgauHYsWP4+OOP4ePjg+3bt8PQ0FB5zMfHB6GhoUhMTNRqDOfOncOECRPQr18/rT1Hp06dtDZ2eQwfPhybN2/G8uXLYW5urmyPjY2Fp6cncnJyKiWOoqIiyGQymJubi/6eENV0nBqgamHevHmQyWT4+uuvVZKAZwwMDDBo0CDlfYVCgejoaDRp0gSGhoawsbHBmDFjcOvWLZXHeXt7o0WLFjhx4gS6desGExMTNGrUCFFRUVAoFAD+VzYvLi5GTEyMsoQOALNmzVL++d+ePebatWvKtgMHDsDb2xtWVlYwNjaGo6Mj3n77bTx58kTZp6ypgXPnzmHw4MGwsLCAkZER3N3dsX79epU+z0roW7ZswfTp02Fvbw9zc3P06tULFy9eLN+bDGDkyJEAgC1btijbsrOz8cMPP2DcuHFlPmb27Nno2LEjLC0tYW5ujjZt2iA2Nhb/vp5Zw4YNcf78eSQlJSnfv2cVlWexb9y4EaGhoahXrx4MDQ1x5cqVUlMD9+/fh4ODAzp37oyioiLl+H///TdMTU3x/vvvl/u1EtFTTASoyispKcGBAwfQtm1bODg4lOsxH3/8MaZNmwYfHx/s2LEDc+bMQWJiIjp37oz79++r9M3IyMDo0aPx3nvvYceOHejXrx/Cw8OxadMmAMCAAQNw7NgxAMA777yDY8eOKe+X17Vr1zBgwAAYGBhg7dq1SExMRFRUFExNTVFYWPjCx128eBGdO3fG+fPnsWTJEmzbtg3NmjWDn58foqOjS/X/9NNPcf36daxZswZff/01Ll++jIEDB6KkpKRccZqbm+Odd97B2rVrlW1btmyBjo4Ohg8f/sLX9uGHH2Lr1q3Ytm0bhg4dikmTJmHOnDnKPgkJCWjUqBE8PDyU79/z0zjh4eG4ceMGVq5ciZ07d8LGxqbUc1lbWyM+Ph4nTpzAtGnTAABPnjzBu+++C0dHR6xcubJcr5OI/kUgquIyMjIEAMKIESPK1T8lJUUAIAQEBKi0//777wIA4dNPP1W2eXl5CQCE33//XaVvs2bNhD59+qi0ARACAwNV2iIiIoSy/hqtW7dOACCkpaUJgiAI33//vQBAOH369EtjByBEREQo748YMUIwNDQUbty4odKvX79+gomJifDw4UNBEATh4MGDAgChf//+Kv22bt0qABCOHTv20ud9Fu+JEyeUY507d04QBEFo37694OfnJwiCIDRv3lzw8vJ64TglJSVCUVGR8NlnnwlWVlaCQqFQHnvRY589X/fu3V947ODBgyrt8+fPFwAICQkJgq+vr2BsbCycOXPmpa+RiMrGigDVOAcPHgSAUovSOnTogKZNm2L//v0q7XZ2dujQoYNKW6tWrXD9+nWNxeTu7g4DAwN88MEHWL9+Pa5evVquxx04cAA9e/YsVQnx8/PDkydPSlUm/j09Ajx9HQDUei1eXl5o3Lgx1q5di7Nnz+LEiRMvnBZ4FmOvXr0gl8uhq6sLfX19zJw5Ew8ePMDdu3fL/bxvv/12uftOnToVAwYMwMiRI7F+/XosXboULVu2LPfjieh/mAhQlWdtbQ0TExOkpaWVq/+DBw8AAHXr1i11zN7eXnn8GSsrq1L9DA0NkZeXV4Foy9a4cWPs27cPNjY2CAwMROPGjdG4cWN89dVXL33cgwcPXvg6nh3/t+dfy7P1FOq8FplMhrFjx2LTpk1YuXIlXF1d0a1btzL7/vHHH+jduzeAp2d1/Pbbbzhx4gSmT5+u9vOW9TpfFqOfnx/y8/NhZ2fHtQFEr4GJAFV5urq66NmzJ06ePFlqsV9Znn0Zpqenlzp2584dWFtbayw2IyMjAEBBQYFK+/PrEACgW7du2LlzJ7Kzs3H8+HF4enoiODgY8fHxLxzfysrqha8DgEZfy7/5+fnh/v37WLlyJcaOHfvCfvHx8dDX18euXbswbNgwdO7cGe3atavQc5a16PJF0tPTERgYCHd3dzx48ABTpkyp0HMSERMBqibCw8MhCAImTJhQ5uK6oqIi7Ny5EwDwxhtvAIBysd8zJ06cQEpKCnr27KmxuJ6tfD9z5oxK+7NYyqKrq4uOHTti+fLlAIA///zzhX179uyJAwcOKL/4n9mwYQNMTEy0dmpdvXr1MHXqVAwcOBC+vr4v7CeTyaCnpwddXV1lW15eHjZu3Fiqr6aqLCUlJRg5ciRkMhl2796NyMhILF26FNu2bXvtsYmkiPsIULXg6emJmJgYBAQEoG3btvj444/RvHlzFBUV4dSpU/j666/RokULDBw4EG5ubvjggw+wdOlS6OjooF+/frh27RpmzJgBBwcHfPLJJxqLq3///rC0tIS/vz8+++wz6OnpIS4uDjdv3lTpt3LlShw4cAADBgyAo6Mj8vPzlSvze/Xq9cLxIyIisGvXLvTo0QMzZ86EpaUlNm/ejJ9++gnR0dGQy+Uaey3Pi4qKemWfAQMGYOHChRg1ahQ++OADPHjwAAsWLCjzFM+WLVsiPj4e3377LRo1agQjI6MKzetHRETg8OHD2LNnD+zs7BAaGoqkpCT4+/vDw8MDTk5Oao9JJGVMBKjamDBhAjp06IBFixZh/vz5yMjIgL6+PlxdXTFq1ChMnDhR2TcmJgaNGzdGbGwsli9fDrlcjr59+yIyMrLMNQEVZW5ujsTERAQHB+O9995D7dq1MX78ePTr1w/jx49X9nN3d8eePXsQERGBjIwM1KpVCy1atMCOHTuUc+xlcXNzw9GjR/Hpp58iMDAQeXl5aNq0KdatW6fWDn3a8sYbb2Dt2rWYP38+Bg4ciHr16mHChAmwsbGBv7+/St/Zs2cjPT0dEyZMwKNHj9CgQQOVfRbKY+/evYiMjMSMGTNUKjtxcXHw8PDA8OHDceTIERgYGGji5RFJgkwQ/rXrBxEREUkK1wgQERFJGBMBIiIiCWMiQEREJGFMBIiIiCSMiQAREZGEMREgIiKSMCYCREREElYjNxS697hY7BCoEhnqMZ+Vkpy8IrFDoEpU36L0LpWaZOwx8dWdyinv1DKNjVWZamQiQEREVC4y/iLBd4CIiEjCWBEgIiLpUuPy1zUVEwEiIpIuTg1waoCIiEjKWBEgIiLp4tQAEwEiIpIwTg1waoCIiEjKWBEgIiLp4tQAEwEiIpIwTg1waoCIiEjKWBEgIiLp4tQAEwEiIpIwTg1waoCIiEjKWBEgIiLp4tQAEwEiIpIwTg1waoCIiEjKWBEgIiLp4tQAEwEiIpIwTg1waoCIiEjKWBEgIiLpYkWAiQAREUmYDtcIMBUiIiKSMFYEiIhIujg1wESAiIgkjKcPcmqAiIhIylgRICIi6eLUABMBIiKSME4NcGqAiIhIylgRICIi6eLUABMBIiKSME4NcGqAiIhIylgRICIi6eLUABMBIiKSME4NiJMIDB06tNx9t23bpsVIiIiIpE2UREAulyv/LAgCEhISIJfL0a5dOwDAyZMn8fDhQ7USBiIiIrVxakCcRGDdunXKP0+bNg3Dhg3DypUroaurCwAoKSlBQEAAzM3NxQiPiIikglMDkAmCIIgZQJ06dXDkyBG4ubmptF+8eBGdO3fGgwcP1B7z3uNiTYVH1YChHjN6KcnJKxI7BKpE9S0MtTq+8YAlGhsr76fJGhurMon+L2hxcTFSUlJKtaekpEChUIgQERERSYZMR3O3akr0swbGjh2LcePG4cqVK+jUqRMA4Pjx44iKisLYsWNFjo6IiGq0avwFrimiJwILFiyAnZ0dFi1ahPT0dABA3bp1ERYWhtDQUJGjIyIiqtlEXyPwbzk5OQDw2osEuUZAWrhGQFq4RkBatL5GYFCMxsbK2/GxxsaqTFXiX9Di4mLs27cPW7Zsgez/V3DeuXMHjx8/FjkyIiKq0bhGQPypgevXr6Nv3764ceMGCgoK4OPjAzMzM0RHRyM/Px8rV64UO0QiIqIaS/QUJigoCO3atUNWVhaMjY2V7W+99Rb2798vYmRERFTjyWSau1VTolcEjhw5gt9++w0GBgYq7Q0aNMDt27dFioqIiCShGpf0NUX0d0ChUKCkpKRU+61bt2BmZiZCRERERNIheiLg4+ODxYsXK+/LZDI8fvwYERER6N+/v3iBERFRzcepAfGnBhYtWoQePXqgWbNmyM/Px6hRo3D58mVYW1tjy5YtYodHREQ1mKwaf4FriuiJgL29PU6fPo0tW7bgzz//hEKhgL+/P0aPHq2yeJCIiIg0r0ptKKQp3FBIWrihkLRwQyFp0faGQqbvrHt1p3LK/b56bosvSkVgx44d5e47aNAgLUZCRESSxpkBcRKBIUOGqNyXyWR4vjDxbN6mrDMKiIiISDNEqakqFArlbc+ePXB3d8fu3bvx8OFDZGdnY/fu3WjTpg0SExPFCI+IiCRCJpNp7FZdib5YMDg4GCtXrkTXrl2VbX369IGJiQk++OADpKSkiBgdERHVZNX5C1xTRF9llZqaCrlcXqpdLpfj2rVrlR8QERGRhIieCLRv3x7BwcFIT09XtmVkZCA0NBQdOnQQMTIiIqrpODVQBaYG1q5di7feegsNGjSAo6MjAODGjRtwdXXF9u3bxQ2uikn4Lh7bv/8W6elPr8Hg1MgZfhM+hmeXbiguKsLXMUtw/Mhh3Ll9C6a1aqFdR098POkTWNexETlyqqg/T57Axri1uJByHvfv3cMXi5bC+41eyuPtWzct83GTP5mC9/38KytM0oD1q1dgQ6zq1VYtLK3w/c8HAQCHD+7Dru3f49KFv5GT/RCrNmyFs2sTMUKtUarzF7imiF4RcHZ2xpkzZ7Br1y5MnjwZkyZNwk8//YSzZ8/C2dlZ7PCqlDq2tvho0idYs3Er1mzcijbtOyI8ZCKupl5Bfn4+Ll1Ige/4j7B283f4fMFXuHn9GqZ9MlHssOk15OXlwdXNDVP/898yj+/ef0jlNmP255DJZOjRq3clR0qa0LBRY3z30wHlbc3mH5TH8vPz0LyVO8YHBIkYIWlDZGQkZDIZgoODlW2CIGDWrFmwt7eHsbExvL29cf78eZXHFRQUYNKkSbC2toapqSkGDRqEW7duqf38olcEgKcZWe/evdG7N//xepmu3Xuo3P8wMAjbv4/H32f/QqMhb2PxijUqxz8J+xQTxoxARvod2NW1r8xQSUO6dO2OLl27v/C4tXUdlfuHfj2Atu07on59B22HRlqgq6sHSyvrMo/59BsIAMi4w6uyapTIBYETJ07g66+/RqtWrVTao6OjsXDhQsTFxcHV1RVz586Fj48PLl68qLwgX3BwMHbu3In4+HhYWVkhNDQUb775Jk6ePAldXd1yxyB6IvDZZ5+99PjMmTMrKZLqpaSkBAf3/YL8vDw0b9W6zD6PHz+GTCaDmZl5JUdHYnjw4D6OHE7CrDmRYodCFXT75nUMe7Mn9PX10aR5K/h/PBn29eqLHVaNJubUwOPHjzF69GisXr0ac+fOVbYLgoDFixdj+vTpGDp0KABg/fr1sLW1xTfffIMPP/wQ2dnZiI2NxcaNG9Gr19Ppwk2bNsHBwQH79u1Dnz59yh2H6IlAQkKCyv2ioiKkpaVBT08PjRs3ZiLwnNTLl/DR2FEoLCyEsbEJ5i1YAqdGpadQCgoKsHLpIvj0HQDTWrVEiJQq2087tsPUxBQ9evqIHQpVQJPmLTFt5ueo79gAWZmZ2Lzua0ye8D5ityRALq8tdnhUDgUFBSgoKFBpMzQ0hKFh2dskBwYGYsCAAejVq5dKIpCWloaMjAyVKrmhoSG8vLxw9OhRfPjhhzh58iSKiopU+tjb26NFixY4evRo9UoETp06VaotJycHfn5+eOutt175+LLe+IIi3Re+8dWdY8OGWLflBzx+9Ai/7t+LzyM+xdLVcSrJQHFREWaFT4GgUCD0PzNEjJYq047t29C3/5s19me/puvYuZvK/WYtW+H9twdgz0878O6oMSJFVfNpsiIQGRmJ2bNnq7RFRERg1qxZpfrGx8fjzz//xIkTJ0ody8jIAADY2tqqtNva2uL69evKPgYGBrCwsCjV59njy0v0xYJlMTc3x2effYYZM179JRYZGQm5XK5y++rL+ZUQpTj09Q1Q36EBmjRrgY8mfYLGrm74bssm5fHioiLM+E8o7ty5hUUr1rAaIBGn/kzG9WtpGDz0HbFDIQ0xNjaBU2MX3L55XexQajRNnj4YHh6O7OxslVt4eHip57x58yaCgoKwadMmGBkZvTS2fxME4ZWJS3n6PE/0isCLPNtu+FXCw8MREhKi0pZTVP5FEtWeIKCosBDA/5KAWzevY8mqdZDXri1ubFRpfkz4AU2bNYerG08nqykKCwtx49pVtHRvI3YoVE4vmwb4t5MnT+Lu3bto27atsq2kpASHDh3CsmXLcPHiRQBPf+uvW7euss/du3eVVQI7OzsUFhYiKytLpSpw9+5ddO7cWa24RU8ElixZonJfEASkp6dj48aN6Nu37ysfX9YbX1BDL0O8atlidOrSDTa2dniSm4t9e3bj1MkT+HLpKhQXF+O/0z7BpQspmL94ORQlJXhw/x4AwFwuh76+gcjRU0U8eZKLmzduKO/fuX0LFy+kQC6XK88Eefz4Mfbv+QXBoWFihUkasHLJAnh29YaNnR0eZmZi07qv8SQ3F336P70Ca052Nu7+k678e33z+jUAgKWV9QvPNKBXE2OxYM+ePXH27FmVtrFjx6JJkyaYNm0aGjVqBDs7O+zduxceHh4AniaGSUlJmD//acW7bdu20NfXx969ezFs2DAAQHp6Os6dO4fo6Gi14hE9EVi0aJHKfR0dHdSpUwe+vr5lllSkLDPzAebM+A8e3L8H01pmaOziii+XrkL7Tp2Rfuc2jiQ93Xhk7Mi3VR63ZNU6tGnHXRqro5Tz5/HReF/l/UULnv4jMGDQEOXZAXsSf4YAAX36DRAlRtKMe3fv4vOZ05D9MAtyC0s0a94SS2M3wfb/E76jh3/FF3P/N106d8bTxG+M/0fwnRAgRsg1gwgnDZiZmaFFixYqbaamprCyslK2BwcHY968eXBxcYGLiwvmzZsHExMTjBo1CsDTbfj9/f0RGhoKKysrWFpaYsqUKWjZsqXyLILykgnPX/+3BrhXQysCVDZDvSq51IW0JCevSOwQqBLVt9Du4lcr3y0aG+vB+pEVfqy3tzfc3d2xePFiAE+r47Nnz8aqVauQlZWFjh07Yvny5SoJRH5+PqZOnYpvvvkGeXl56NmzJ1asWAEHB/X2EWEiQNUeEwFpYSIgLdpOBKz94jU21v24ERobqzKJMjXwbIOE8ti2bZsWIyEiIinjtQZESgTKuuwwERERVT5REoF169aJ8bREREQqWBGoAhsKpaWl4fLly6XaL1++jGvXrlV+QEREJB0yDd6qKdETAT8/Pxw9erRU+++//w4/P7/KD4iIiEhCRE8ETp06hS5dupRq79SpE06fPl35ARERkWRocovh6kr0DYVkMhkePXpUqj07OxslJSUiRERERFJRnb/ANUX0ikC3bt0QGRmp8qVfUlKCyMhIdO3aVcTIiIiIaj7RKwLR0dHo3r073Nzc0K3b08twHj58GDk5OThw4IDI0RERUU3GikAVqAg0a9YMZ86cwbBhw3D37l08evQIY8aMwYULF0rtxUxERKRJXCNQBSoCAGBvb4958+aJHQYREZHkiF4RWLduHb777rtS7d999x3Wr18vQkRERCQZ3EdA/EQgKioK1talr6VtY2PDKgEREWkVpwaqQCJw/fp1ODk5lWpv0KABbty4IUJERERE0iF6ImBjY4MzZ86Uav/rr79gZWUlQkRERCQVrAhUgcWCI0aMwOTJk2FmZobu3bsDAJKSkhAUFIQRI6rntZ2JiKh6qM5f4JoieiIwd+5cXL9+HT179oSe3tNwSkpK4OvryzUCREREWiZ6ImBgYIBvv/0WU6ZMQVpaGkxMTNCyZUs0aNBA7NCIiKimY0FA3ETg4cOHmD59Or799ltkZWUBACwsLDBixAjMnTsXtWvXFjM8IiKq4Tg1IGIikJmZCU9PT9y+fRujR49G06ZNIQgCUlJSEBcXh/379+Po0aOwsLAQK0QiIqIaT7RE4LPPPoOBgQFSU1Nha2tb6ljv3r3x2WefYdGiRSJFSERENR0rAiKePrh9+3YsWLCgVBIAAHZ2doiOjkZCQoIIkRERkVTw9EERE4H09HQ0b978hcdbtGiBjIyMSoyIiIhIekRLBKytrXHt2rUXHk9LS+OGQkREpFWsCIiYCPTt2xfTp09HYWFhqWMFBQWYMWMG+vbtK0JkREQkGbzokHiLBWfPno127drBxcUFgYGBaNKkCQDg77//xooVK1BQUICNGzeKFR4REZEkiJYI1K9fH8eOHUNAQADCw8MhCAKAp2UaHx8fLFu2DA4ODmKFR0REElCdS/qaIuqGQk5OTti9ezeysrJw+fJlAICzszMsLS3FDIuIiCSCiUAV2GIYeLqbYIcOHcQOg4iISHKqRCJAREQkBhYEmAgQEZGEcWpAxNMHiYiISHysCBARkWSxIMBEgIiIJIxTA5waICIikjRWBIiISLJYEGAiQEREEqajw0yAUwNEREQSxooAERFJFqcGWBEgIiKSNFYEiIhIsnj6IBMBIiKSMOYBnBogIiKSNFYEiIhIsjg1wESAiIgkjIkApwaIiIgkjRUBIiKSLBYEmAgQEZGEcWqAUwNERESSxooAERFJFgsCTASIiEjCODXAqQEiIiJJY0WAiIgkiwUBJgJERCRhnBrg1AAREZGksSJARESSxYIAEwEiIpIwTg1waoCIiEjSamRFwNRQV+wQqBJdv/dE7BCoEjWsYyp2CFSDsCBQQxMBIiKi8uDUAKcGiIiIJI0VASIikiwWBJgIEBGRhHFqgFMDREREksaKABERSRYLAkwEiIhIwjg1wKkBIiIiSWNFgIiIJIsVASYCREQkYcwDODVAREQkaUwEiIhIsmQymcZu6oiJiUGrVq1gbm4Oc3NzeHp6Yvfu3crjgiBg1qxZsLe3h7GxMby9vXH+/HmVMQoKCjBp0iRYW1vD1NQUgwYNwq1bt9R+D5gIEBGRZMlkmrupo379+oiKikJycjKSk5PxxhtvYPDgwcov++joaCxcuBDLli3DiRMnYGdnBx8fHzx69Eg5RnBwMBISEhAfH48jR47g8ePHePPNN1FSUqLeeyAIgqBe+FXfk6Ia95LoJXj1QWnh1QelxVhfu+P3+OqoxsY6GNT5tR5vaWmJL774AuPGjYO9vT2Cg4Mxbdo0AE9/+7e1tcX8+fPx4YcfIjs7G3Xq1MHGjRsxfPhwAMCdO3fg4OCAn3/+GX369Cn387IiQEREkqXJqYGCggLk5OSo3AoKCl4ZQ0lJCeLj45GbmwtPT0+kpaUhIyMDvXv3VvYxNDSEl5cXjh59mricPHkSRUVFKn3s7e3RokULZZ/yYiJARESSpcmpgcjISMjlcpVbZGTkC5/77NmzqFWrFgwNDfHRRx8hISEBzZo1Q0ZGBgDA1tZWpb+tra3yWEZGBgwMDGBhYfHCPuXF0weJiIg0IDw8HCEhISpthoaGL+zv5uaG06dP4+HDh/jhhx/g6+uLpKQk5fHnFyAKgvDKRYnl6fM8JgJERCRZOhrcSMDQ0PClX/zPMzAwgLOzMwCgXbt2OHHiBL766ivluoCMjAzUrVtX2f/u3bvKKoGdnR0KCwuRlZWlUhW4e/cuOndWb60CpwaIiEiyxDproCyCIKCgoABOTk6ws7PD3r17lccKCwuRlJSk/JJv27Yt9PX1Vfqkp6fj3LlzaicCrAgQERFVsk8//RT9+vWDg4MDHj16hPj4ePz6669ITEyETCZDcHAw5s2bBxcXF7i4uGDevHkwMTHBqFGjAAByuRz+/v4IDQ2FlZUVLC0tMWXKFLRs2RK9evVSKxYmAkREJFliXWvgn3/+wfvvv4/09HTI5XK0atUKiYmJ8PHxAQCEhYUhLy8PAQEByMrKQseOHbFnzx6YmZkpx1i0aBH09PQwbNgw5OXloWfPnoiLi4Ourq5asXAfAar2uI+AtHAfAWnR9j4C/WJ+19hYuz/uqLGxKhPXCBAREUkYpwaIiEiyeBliJgJERCRhzAM4NUBERCRprAgQEZFkycCSABMBIiKSLB3mAZwaICIikjJWBIiISLJ41kA5E4EdO3aUe8BBgwZVOBgiIqLKxDygnInAkCFDyjWYTCZDSUnJ68RDRERElahciYBCodB2HERERJVOk5chrq5ea41Afn4+jIyMNBULERFRpWIeUIGzBkpKSjBnzhzUq1cPtWrVwtWrVwEAM2bMQGxsrMYDJCIiIu1ROxH4/PPPERcXh+joaBgYGCjbW7ZsiTVr1mg0OCIiIm2SyWQau1VXaicCGzZswNdff43Ro0erXPO4VatWuHDhgkaDIyIi0iaZTHO36krtROD27dtwdnYu1a5QKFBUVKSRoIiIiKhyqJ0ING/eHIcPHy7V/t1338HDw0MjQREREVUGHZlMY7fqSu2zBiIiIvD+++/j9u3bUCgU2LZtGy5evIgNGzZg165d2oiRiIhIK6rv17fmqF0RGDhwIL799lv8/PPPkMlkmDlzJlJSUrBz5074+PhoI0YiIiLSkgrtI9CnTx/06dNH07EQERFVquq82l9TKryhUHJyMlJSUiCTydC0aVO0bdtWk3ERERFpHS9DXIFE4NatWxg5ciR+++031K5dGwDw8OFDdO7cGVu2bIGDg4OmYyQiIiItUXuNwLhx41BUVISUlBRkZmYiMzMTKSkpEAQB/v7+2oiRiIhIK7ihUAUqAocPH8bRo0fh5uambHNzc8PSpUvRpUsXjQZHRESkTdX4+1tj1K4IODo6lrlxUHFxMerVq6eRoIiIiKhyqJ0IREdHY9KkSUhOToYgCACeLhwMCgrCggULNB4gERGRtnBqoJxTAxYWFiovMjc3Fx07doSe3tOHFxcXQ09PD+PGjcOQIUO0EigREZGm8ayBciYCixcv1nIYREREJIZyJQK+vr7ajoOIiKjSVeeSvqZUeEMhAMjLyyu1cNDc3LzC4125cgWpqano3r07jI2NIQgCPyQiItIafsNUYLFgbm4uJk6cCBsbG9SqVQsWFhYqt4p48OABevXqBVdXV/Tv3x/p6ekAgPHjxyM0NLRCYxIREdGrqZ0IhIWF4cCBA1ixYgUMDQ2xZs0azJ49G/b29tiwYUOFgvjkk0+gp6eHGzduwMTERNk+fPhwJCYmVmhMIiKiV+FliCswNbBz505s2LAB3t7eGDduHLp16wZnZ2c0aNAAmzdvxujRo9UOYs+ePfjll19Qv359lXYXFxdcv35d7fGIiIjKoxp/f2uM2hWBzMxMODk5AXi6HiAzMxMA0LVrVxw6dKhCQeTm5qpUAp65f/8+DA0NKzQmERERvZraiUCjRo1w7do1AECzZs2wdetWAE8rBc8uQqSu7t27q0wryGQyKBQKfPHFF+jRo0eFxiQiInoVbihUgamBsWPH4q+//oKXlxfCw8MxYMAALF26FMXFxVi4cGGFgvjiiy/g7e2N5ORkFBYWIiwsDOfPn0dmZiZ+++23Co1JRET0KtX4+1tjZMKzfYIr6MaNG0hOTkbjxo3RunXrCo+TkZGBmJgYnDx5EgqFAm3atEFgYCDq1q2r9lhPil7rJVUrK5cvxaqY5SptVlbW2Jd0RKSIKt/1e0/EDkErSkqKER+3Ckn7duNh5gNYWFnjjT4D8e7746Gj87SYtyVuJY4c2IP79zKgp6ePxq5N8Z5/IFybtRQ5eu1pWMdU7BBEE7t6FZZ+tRCj3huDsP9MFzucSmGsr93xP/z+vMbGWvVOc42NVZleax8B4OlFiBwdHXHz5k2MGzcOa9eurdA4dnZ2mD179uuGI0mNnV2wcs3/3ncdHV0RoyFN2bYlDok7fkDQf2bDwakxUi/+jSXzZ8HEtBYGvjMKAGBfvwE+CJoG27r1UFhQgB3fb8assEDEbPoR8toVO52XqqZzZ8/gh++/haur26s7U7lV59X+mvLaicAzmZmZWL9+fYUTgaysLMTGxiIlJQUymQxNmzbF2LFjYWlpqakQayxdXV1YW9cROwzSsIvnz6BDFy+08+wGALC1s8eh/Ym4culvZR+vXv1UHjMuIAT7ft6Oa6mX0Lptx0qNl7TnyZNcfPqfqZg5ay5Wr4oRO5wahXlABRYLakNSUhKcnJywZMkSZGVlITMzE0uWLIGTkxOSkpLEDq/Ku3HjOnx6dMOAPj0xbUoIbt28KXZIpAFNW3rgzJ9/4PbNp6fQpl25hJRzp9G2Y9cy+xcVFWHPrm0wMa0FJ2fXygyVtGze3M/QrbsXOnl2FjsUqoE0VhF4HYGBgRg2bBhiYmKgq/u0rF1SUoKAgAAEBgbi3LlzL3xsQUEBCgoKVNpKdAwkc9phi1atMWdeFBo0aIgHDx5gzaoY+L03Et//uBO1WRqu1oaO9MOT3MeY6DsUOjq6UChKMNo/EN179lXpd+LYIXz5WTgKCvJhYWWN2QtiYC7nZ19TJP78Ey6k/I3N8d+LHUqNVJ1X+2tKlagIpKamIjQ0VJkEAE/L3SEhIUhNTX3pYyMjIyGXy1VuC+ZHajvkKqNrt+7o5dMHLq5u6OTZGUtXrAIA7Pxxu7iB0Ws7cnAPft37M0L+Ow9ffr0Zk/8zGz9u3YgDiTtV+rV0b49Fa7Ygatk6eLTvjC9mT8PDrEyRoiZNykhPR3TU5/g88gvJ/HJT2XQ0eKuuyl0RGDp06EuPP3z4sMJBtGnTBikpKXBzU10Ek5KSAnd395c+Njw8HCEhISptJToGFY6lujM2MYGziytucEfGai9u5WK8PdIP3d7oAwBo2MgF9/7JwA/frMMbfQcq+xkZG6NuPUfUrecIt2at8PF7g7Hv5+14Z/Q4sUInDfn77/PIzHyAUcP/9+9vSUkJ/jx5At9u2Yw//jyr8gsUUUWUOxGQy+WvPD5mzJgKBTF58mQEBQXhypUr6NSpEwDg+PHjWL58OaKionDmzBll31atWqk81tDQsFSmLKXTB59XWFiItLRUeLRtK3Yo9JoKC/Ih01H9PUNHRweCoHjp4wRBQFFRoTZDo0rSsVMnfJ+gWgGa+d9wODk1wlj/CUwCNIBTA2okAuvWrdNaECNHjgTw9IJGZR2TyWTKSxKXlJRoLY7qaOEX89Hduwfq1rVHZubTNQK5jx9j4OAhYodGr6mdZ3d8vykWdWzs4ODUGGmXL2DHd5vQs99gAEB+Xh6+27QGHbp4wcLSGo9ysrH7x+/w4N5ddPHyETl60gRT01pwdlFd+GlsbAJ57dql2qlidJgHVI3FgmlpaWKHUG39888/CA8LxcOsh7CwtEDLVq2x/ptvYW9fT+zQ6DV9MDkMm9euwKqvIpGdlQUL6zroM/BtDBvzAQBAR1cHt29ew/yIXcjJfggzczlc3Jpj3pJYODo1Fjl6IqouXntnwapIylMDUlRTdxakskl5Z0Ep0vbOgiE7LmhsrIWDmmhsrMokWkVgx44d6NevH/T19bFjx46X9h00aFAlRUVERFLCNQIiJgJDhgxBRkYGbGxsMGTIkBf247oAIiIi7REtEVAoFGX++Xm3bt2qjHCIiEiCuFiwgnsgbNy4EV26dIG9vT2u///56osXL8aPP/6oscAyMjIwefJkuLi4aGxMIiKif5PJNHerrtROBGJiYhASEoL+/fvj4cOHyrJ97dq1sXjxYrXGevjwIUaPHo06derA3t4eS5YsgUKhwMyZM9GoUSMcO3aswhcxIiIioldTOxFYunQpVq9ejenTp6tsZtGuXTucPXtWrbE+/fRTHDp0CL6+vrC0tMQnn3yCN998E0eOHMHu3btx4sQJ5R4DREREmqYjk2nsVl2pvUYgLS0NHh4epdoNDQ2Rm5ur1lg//fQT1q1bh169eiEgIADOzs5wdXVVu7JARERUEdX5GgGaovZ74OTkhNOnT5dq3717N5o1a6bWWHfu3FE+plGjRjAyMsL48ePVDYmIiIgqSO2KwNSpUxEYGIj8/HwIgoA//vgDW7ZsQWRkJNasWaPWWAqFAvr6/9stQldXF6am3CyEiIgqRzWu6GuM2onA2LFjUVxcjLCwMDx58gSjRo1CvXr18NVXX2HEiBFqjSUIAvz8/JQXDcrPz8dHH31UKhnYtm2bumESERG9UnWe29eUCu0jMGHCBEyYMAH379+HQqGAjY1NhZ7c19dX5f57771XoXGIiIioYl5rQyFra+vXenJtXtGQiIjoVVgQqEAi4OTk9NK9ma9evfpaAREREVUW7ixYgUQgODhY5X5RURFOnTqFxMRETJ06VVNxERERUSVQOxEICgoqs3358uVITk5+7YCIiIgqCxcLanAvhX79+uGHH37Q1HBERERax2sNaDAR+P7772Fpaamp4YiIiKgSqD014OHhobJYUBAEZGRk4N69e1ixYoVGgyMiItImLhasQCIwZMgQlfs6OjqoU6cOvL290aRJE03FRUREpHUyMBNQKxEoLi5Gw4YN0adPH9jZ2WkrJiIiIqokaq0R0NPTw8cff4yCggJtxUNERFRpdGSau1VXai8W7NixI06dOqWNWIiIiCoVE4EKrBEICAhAaGgobt26hbZt25a6QFCrVq00FhwRERFpV7krAuPGjUNOTg6GDx+OtLQ0TJ48GV26dIG7uzs8PDyU/yciIqouZDKZxm7qiIyMRPv27WFmZgYbGxsMGTIEFy9eVOkjCAJmzZoFe3t7GBsbw9vbG+fPn1fpU1BQgEmTJsHa2hqmpqYYNGgQbt26pVYs5U4E1q9fj/z8fKSlpZW6Xb16Vfl/IiKi6kKsqYGkpCQEBgbi+PHj2Lt3L4qLi9G7d2/k5uYq+0RHR2PhwoVYtmwZTpw4ATs7O/j4+ODRo0fKPsHBwUhISEB8fDyOHDmCx48f480330RJSUm5Y5EJgiCUp6OOjg4yMjIqfMnhyvSkqFwviWqI6/eeiB0CVaKGdUxf3YlqDGN97Y7/ZZLmfoGd2KleqcX0hoaGMDQ0fOVj7927BxsbGyQlJaF79+4QBAH29vYIDg7GtGnTADz97d/W1hbz58/Hhx9+iOzsbNSpUwcbN27E8OHDAQB37tyBg4MDfv75Z/Tp06dccau1WFDd0gcREVFVpskthiMjIyGXy1VukZGR5YojOzsbAJQ79KalpSEjIwO9e/dW9jE0NISXlxeOHj0KADh58iSKiopU+tjb26NFixbKPuWh1mJBV1fXVyYDmZmZ6gxJREQkGk1edCg8PBwhISEqbeWpBgiCgJCQEHTt2hUtWrQAAGRkZAAAbG1tVfra2tri+vXryj4GBgawsLAo1efZ48tDrURg9uzZkMvl6jyEiIhIEso7DfC8iRMn4syZMzhy5EipY8//8i0Iwit/IS9Pn39TKxEYMWJEtVgjQEREVB5in/8/adIk7NixA4cOHUL9+vWV7c92783IyEDdunWV7Xfv3lVWCezs7FBYWIisrCyVqsDdu3fRuXPncsdQ7jUCXB9AREQ1jViXIRYEARMnTsS2bdtw4MABODk5qRx3cnKCnZ0d9u7dq2wrLCxEUlKS8ku+bdu20NfXV+mTnp6Oc+fOqZUIlLsiUM6TC4iIiOgVAgMD8c033+DHH3+EmZmZck5fLpfD2NgYMpkMwcHBmDdvHlxcXODi4oJ58+bBxMQEo0aNUvb19/dHaGgorKysYGlpiSlTpqBly5bo1atXuWMpdyKgUCjUfJlERERVm45IVx+MiYkBAHh7e6u0r1u3Dn5+fgCAsLAw5OXlISAgAFlZWejYsSP27NkDMzMzZf9FixZBT08Pw4YNQ15eHnr27Im4uDjo6uqWO5Zy7yNQnXAfAWnhPgLSwn0EpEXb+wisOHpNY2MFdG6osbEqk9oXHSIiIqKaQ+2LDhEREdUUYp81UBUwESAiIsnS5IZC1RWnBoiIiCSMFQEiIpIsFgSYCBARkYRxaoBTA0RERJLGigAREUkWCwJMBIiISMJYFud7QEREJGmsCBARkWTxyrpMBIiISMKYBnBqgIiISNJYESAiIsniPgJMBIiISMKYBnBqgIiISNJYESAiIsnizAATASIikjCePsipASIiIkljRYCIiCSLvw0zESAiIgnj1ACTISIiIkljRYCIiCSL9QAmAkREJGGcGqihiQC3jJQWJxtTsUOgSmTRfqLYIVAlyju1TOwQarwamQgQERGVBxfKMREgIiIJ49QAkyEiIiJJY0WAiIgki/UAJgJERCRhnBng1AAREZGksSJARESSpcPJASYCREQkXZwa4NQAERGRpLEiQEREkiXj1AATASIiki5ODXBqgIiISNJYESAiIsniWQNMBIiISMI4NcCpASIiIkljRYCIiCSLFQEmAkREJGE8fZBTA0RERJLGigAREUmWDgsCTASIiEi6ODXAqQEiIiJJY0WAiIgki2cNMBEgIiIJ49QApwaIiIgkjRUBIiKSLJ41wESAiIgkjFMDnBogIiKSNFYEiIhIsnjWABMBIiKSMOYBnBogIiKSNFYEiIhIsnQ4N8BEgIiIpItpAKcGiIiIJI0VASIiki6WBJgIEBGRdHFDIU4NEBERSRorAkREJFk8aYCJABERSRjzAE4NEBERSRorAkREJF0sCTARICIi6eJZA5waICIikjRWBIiISLJ41gArAkRERJJWpRKBK1eu4JdffkFeXh4AQBAEkSMiIqKaTKbBmzoOHTqEgQMHwt7eHjKZDNu3b1c5LggCZs2aBXt7exgbG8Pb2xvnz59X6VNQUIBJkybB2toapqamGDRoEG7duqVmJFUkEXjw4AF69eoFV1dX9O/fH+np6QCA8ePHIzQ0VOToiIioxhIpE8jNzUXr1q2xbNmyMo9HR0dj4cKFWLZsGU6cOAE7Ozv4+Pjg0aNHyj7BwcFISEhAfHw8jhw5gsePH+PNN99ESUmJWrFUiUTgk08+gZ6eHm7cuAETExNl+/Dhw5GYmChiZERERJrXr18/zJ07F0OHDi11TBAELF68GNOnT8fQoUPRokULrF+/Hk+ePME333wDAMjOzkZsbCy+/PJL9OrVCx4eHti0aRPOnj2Lffv2qRVLlUgE9uzZg/nz56N+/foq7S4uLrh+/bpIURERUU0n0+B/BQUFyMnJUbkVFBSoHVNaWhoyMjLQu3dvZZuhoSG8vLxw9OhRAMDJkydRVFSk0sfe3h4tWrRQ9imvKpEI5ObmqlQCnrl//z4MDQ1FiIiIiKRAJtPcLTIyEnK5XOUWGRmpdkwZGRkAAFtbW5V2W1tb5bGMjAwYGBjAwsLihX3Kq0okAt27d8eGDRuU92UyGRQKBb744gv06NFDxMiIiIjKJzw8HNnZ2Sq38PDwCo8ne+7cRkEQSrU9rzx9nlcl9hH44osv4O3tjeTkZBQWFiIsLAznz59HZmYmfvvtN7HDIyKiGkqT2wgYGhpqpIptZ2cH4Olv/XXr1lW23717V1klsLOzQ2FhIbKyslSqAnfv3kXnzp3Ver4qURFo1qwZzpw5gw4dOsDHxwe5ubkYOnQoTp06hcaNG4sdHhER1VRinT/4Ek5OTrCzs8PevXuVbYWFhUhKSlJ+ybdt2xb6+voqfdLT03Hu3Dm1E4EqUREAnmY3s2fPFjsMIiIirXv8+DGuXLmivJ+WlobTp0/D0tISjo6OCA4Oxrx58+Di4gIXFxfMmzcPJiYmGDVqFABALpfD398foaGhsLKygqWlJaZMmYKWLVuiV69easVSZRKBrKwsxMbGIiUlBTKZDE2bNsXYsWNhaWkpdmhERFRDiXXRoeTkZJU1cCEhIQAAX19fxMXFISwsDHl5eQgICEBWVhY6duyIPXv2wMzMTPmYRYsWQU9PD8OGDUNeXh569uyJuLg46OrqqhWLTKgC2/clJSVh8ODBMDc3R7t27QA8PTXi4cOH2LFjB7y8vNQaL79YG1ESUVVg0X6i2CFQJco7VfaGO5py9tZjjY3Vsn4tjY1VmapERSAwMBDDhg1DTEyMMpMpKSlBQEAAAgMDce7cOZEjJCIiqpmqxGLB1NRUhIaGqpQzdHV1ERISgtTUVBEjIyKimqwKrhWsdFUiEWjTpg1SUlJKtaekpMDd3b3yAyIiImlgJlA1pgYmT56MoKAgXLlyBZ06dQIAHD9+HMuXL0dUVBTOnDmj7NuqVSuxwqySYlevwv69e5CWdhWGRkZwd/dAcMgUNHRqJHZopAUnk08gbm0sUv4+h3v37mHRkuV4o6d6K4SpapoyrjfmTBqEZZsPYuqCHwC8eH7800UJWLRhPwDAqb41oj55C54ejWCor4e9R1MQMv873M18VOZjiZ5XJRKBkSNHAgDCwsLKPCaTyZS7Jal7VaWaLvnEHxg+cjSat2yJkuISLF2yCB9N8Me2HT+VuW0zVW95eU/g5uaGwW8NRWjwJLHDIQ1p28wR/kM748wl1UvINuyluitd7y7NsTJiFBL2nwYAmBgZYNeKQJy9dBv9PlgKAIgIGIAfvvoQ3cd8yUu5l4NYZw1UJVUiEUhLSxM7hGor5utYlfufzY1Ej26eSPn7PNq2ay9SVKQtXbt5oWs39c6ioarN1NgA6+b5IWDOFvxnfF+VY/88UP2tfqB3SySduIxrtx8AADzdG6GBvRU6jZyPR7n5AIAPIjYh/dAX8O7gioO/X6ycF1GNqbkbb41UJRKBBg0aiB1CjfH4/69VbS6XixwJEZXH4vDhSDx8Dgd/v1gqEfg3G0sz9O3aAhNmblS2GRroQRAEFBT+75zp/MJilJQo0Nm9MRMBKhfREoEdO3agX79+0NfXx44dO17ad9CgQZUUVfUmCAIWREfCo01buLi4ih0OEb3Cu33awr2JA7q+F/3Kvu8N7IhHT/Kx/cBpZdsfZ68hN68QnwcNxsxlOyCDDJ8HDYaurg7srM21GHnNwYKAiInAkCFDkJGRARsbGwwZMuSF/V61LqCgoKDU9Z4FXc1c+KG6iZz7GS5fuoS4jd+IHQoRvUJ929r4YurbGBiwXOU3+hcZM7gTvt2drNL3ftZjjA6LxZJPhyNgpBcUCgFbE0/iz79voESh0Gb4NQczAfESAcW/fkgVr/EDGxkZWeoaBdNnROC/M2dVeMzqKPLzOfj11wNYu34TbP//ylVEVHV5NHWErZU5jm7+3yJpPT1ddG3TGB8N7w55x2AoFE8X+3XxaAw3Jzu8/591pcbZf/wCmg+aDavapiguViD7cR7S9s7D9f9fR0D0KqKvESgqKkLv3r2xatUquLqqX84ODw9X7tH8jKArnWqAIAiI/HwODuzfi9i4jahf30HskIioHA7+cRFt3/lcpe3r2e/hYto/+DJurzIJAADfIZ44+fcNnL10+4XjPXiYCwDwau8KG8ta2JV0VjuB1zA8a6AKJAL6+vo4d+4cZBVculnW9Z+ldK2BeXNmY/fPu7B46QqYmpji/r17AIBaZmYwMjISOTrStCe5ubhx44by/u1bt3AhJQVyuRx17e1FjIzU9fhJAf5OTVdpy80rRGZ2rkq7makRhvp44D8LE8oc5/1BnXAxLQP3sh6jYysnLJj6DpZuPojL1+9qNf6agmcNVIFEAADGjBmD2NhYREVFiR1KtbP12y0AAH+/91XaP5sbicFvDRUjJNKi8+fPYfzYMcr7C6IjAQCDBr+FOfP496cmerdPW8ggw9bE5DKPuza0wWeTBsFSboLrdzIRHfsLlmw6UMlRUnVWJa4+OGnSJGzYsAHOzs5o164dTE1NVY4vXLhQrfGkVBEgkhpefVBatH31wUsZTzQ2lqtd9dzErUpUBM6dO4c2bdoAAC5duqRyrKJTBkRERK/Er5iqkQgcPHhQ7BCIiIgkqUokAkRERGLgWQNVJBHIzc1FVFQU9u/fj7t375baV+Dq1asiRUZERDUZZ5+rSCIwfvx4JCUl4f3330fdunW5LoCIiKiSVIlEYPfu3fjpp5/QpUsXsUMhIiIJ4a+dVSQRsLCwgKWlpdhhEBGR1DATgI7YAQDAnDlzMHPmTDx5ornzOYmIiOjVqkRF4Msvv0RqaipsbW3RsGFD6Ovrqxz/888/RYqMiIhqMp41UEUSgcGDB3OBIBERVTp+9VSRRGDWrFlih0BERCRJoq4R0NHRga6ubqmbhYUFOnXqhG3btokZHhER1XAyDd6qK1ErAgkJZV9W8+HDh/jjjz/w3nvvYf369Xj33XcrOTIiIpKE6vwNriFV4uqDL7J8+XJs2LABv//+u1qP49UHiWouXn1QWrR99cFrD/I1NlZDKyONjVWZqsTpgy/Su3fvUlcjJCIi0hSZBv+rrqrEYsEXycvLg5FR9cywiIio6uNZA1W8IrB69Wp4eHiIHQYREVGNJWpFICQkpMz27OxsJCcnIzU1FYcPH67kqIiISCpYEBA5ETh16lSZ7ebm5ujbty8CAgLQoEGDSo6KiIikglMDIicCBw8eFPPpiYiIJK9KLxYkIiLSLpYEmAgQEZFkcWqgip81QERERNrFigAREUkWCwJMBIiISMI4NcCpASIiIkljRYCIiCSrOl8jQFOYCBARkXQxD+DUABERkZSxIkBERJLFggATASIikjCeNcCpASIiIkljRYCIiCSLZw0wESAiIiljHsCpASIiIiljRYCIiCSLBQEmAkREJGE8a4BTA0RERJLGigAREUkWzxpgIkBERBLGqQFODRAREUkaEwEiIiIJ49QAERFJFqcGWBEgIiKSNFYEiIhIsnjWABMBIiKSME4NcGqAiIhI0lgRICIiyWJBgIkAERFJGTMBTg0QERFJGSsCREQkWTxrgIkAERFJGM8a4NQAERGRpLEiQEREksWCABMBIiKSMmYCnBogIiKSMlYEiIhIsnjWABMBIiKSMJ41wKkBIiIiSZMJgiCIHQS9voKCAkRGRiI8PByGhoZih0Naxs9bWvh5kzYxEaghcnJyIJfLkZ2dDXNzc7HDIS3j5y0t/LxJmzg1QEREJGFMBIiIiCSMiQAREZGEMRGoIQwNDREREcGFRBLBz1ta+HmTNnGxIBERkYSxIkBERCRhTASIiIgkjIkAERGRhDERqKb8/PwwZMiQ1x5HJpNh+/btVSIWejFNfE5Uc/DngTSJiYCW3Lx5E/7+/rC3t4eBgQEaNGiAoKAgPHjwQOzQqIrx8/ODTCaDTCaDnp4eHB0d8fHHHyMrK0vZJz09Hf369RMxSlKHt7c3goODS7Vv374dMg1c5YY/D6RJTAS04OrVq2jXrh0uXbqELVu24MqVK1i5ciX2798PT09PZGZmih0iVTF9+/ZFeno6rl27hjVr1mDnzp0ICAhQHrezs+OpY4TCwkIA/HkgzWIioAWBgYEwMDDAnj174OXlBUdHR/Tr1w/79u3D7du3MX36dABPLyQSFhYGBwcHGBoawsXFBbGxscpxzp8/jwEDBsDc3BxmZmbo1q0bUlNTy3zOhg0bYvHixSpt7u7umDVrlvL+5cuX0b17dxgZGaFZs2bYu3dvqXFu376N4cOHw8LCAlZWVhg8eDCuXbumPF5SUoKQkBDUrl0bVlZWCAsLA89AfX2Ghoaws7ND/fr10bt3bwwfPhx79uxRHv93KfjatWuQyWSIj49H586dYWRkhObNm+PXX39VGfPvv/9G//79UatWLdja2uL999/H/fv3K/FV0cvMmjUL7u7u2LhxIxo2bAi5XI4RI0bg0aNHyj7e3t6YOHEiQkJCYG1tDR8fHwD8eSDNYiKgYZmZmfjll18QEBAAY2NjlWN2dnYYPXo0vv32WwiCgDFjxiA+Ph5LlixBSkoKVq5ciVq1agF4+oX87Ev7wIEDOHnyJMaNG4fi4uIKxaVQKDB06FDo6uri+PHjWLlyJaZNm6bS58mTJ+jRowdq1aqFQ4cO4ciRI6hVqxb69u2r/E3kyy+/xNq1axEbG4sjR44gMzMTCQkJFYqJynb16lUkJiZCX1//pf2mTp2K0NBQnDp1Cp07d8agQYOUU0/p6enw8vKCu7s7kpOTkZiYiH/++QfDhg2rjJdA5ZSamort27dj165d2LVrF5KSkhAVFaXSZ/369dDT08Nvv/2GVatWvXAs/jxQhQmkUcePHxcACAkJCWUeX7hwoQBA+P333wUAwt69e8vsFx4eLjg5OQmFhYVlHvf19RUGDx6svN+gQQNh0aJFKn1at24tRERECIIgCL/88ougq6sr3Lx5U3l89+7dKrHGxsYKbm5ugkKhUPYpKCgQjI2NhV9++UUQBEGoW7euEBUVpTxeVFQk1K9fXyUWUo+vr6+gq6srmJqaCkZGRgIAAYCwcOFCZZ9/f05paWkCgDI/h/nz5wuCIAgzZswQevfurfI8N2/eFAAIFy9e1P6LkjgvLy8hKCioVHtCQoLw7J/diIgIwcTERMjJyVEenzp1qtCxY0eVcdzd3UuNw58H0iQ9UbIPCRP+v4yelpYGXV1deHl5ldnv9OnT6Nat2yt/KyyvlJQUODo6on79+so2T09PlT4nT57ElStXYGZmptKen5+P1NRUZGdnIz09XeVxenp6aNeuHacHXlOPHj0QExODJ0+eYM2aNbh06RImTZr00seU9TmkpKQAePpZHjx4UFlh+rfU1FS4urpq9gVQhTRs2FDl71vdunVx9+5dlT7t2rUr11j8eaCKYiKgYc7OzpDJZPj777/LPKXuwoULsLCwgImJyUvHeX5a4VV0dHRKfRkXFRUp/1zWF/Xzq5cVCgXatm2LzZs3l+pbp04dteIh9ZiamsLZ2RkAsGTJEvTo0QOzZ8/GnDlz1Brn2WeqUCgwcOBAzJ8/v1SfunXrvn7A9FLm5ubIzs4u1f7w4UOYm5sr7z+f6MtkMigUCpU2U1PTCsfBnwcqD64R0DArKyv4+PhgxYoVyMvLUzmWkZGBzZs3Y/jw4WjZsiUUCgWSkpLKHKdVq1Y4fPiwypf5y9SpUwfp6enK+zk5OUhLS1Peb9asGW7cuIE7d+4o244dO6YyRps2bXD58mXY2NjA2dlZ5SaXyyGXy1G3bl0cP35c+Zji4mKcPHmyXDFS+UVERGDBggUqn9fzyvocmjRpAuDpZ3n+/Hk0bNiw1Gf5Ol8sVD5NmjRBcnJyqfYTJ07Azc1NK8/JnweqKCYCWrBs2TIUFBSgT58+OHToEG7evInExET4+PigXr16+Pzzz9GwYUP4+vpi3Lhx2L59O9LS0vDrr79i69atAICJEyciJycHI0aMQHJyMi5fvoyNGzfi4sWLZT7nG2+8gY0bN+Lw4cM4d+4cfH19oaurqzzeq1cvuLm5YcyYMfjrr79w+PBh5dkLz4wePRrW1tYYPHgwDh8+jLS0NCQlJSEoKAi3bt0CAAQFBSEqKgoJCQm4cOECAgIC8PDhQ+28kRLm7e2N5s2bY968eS/ss3z5cuXnEBgYiKysLIwbNw7A0zNXMjMzMXLkSPzxxx+4evUq9uzZg3HjxqGkpKSyXoZkBQQEIDU1FYGBgfjrr79w6dIlLF++HLGxsZg6dapWnpM/D1RRTAS0wMXFBcnJyWjcuDGGDx+Oxo0b44MPPkCPHj1w7NgxWFpaAgBiYmLwzjvvICAgAE2aNMGECROQm5sL4Gll4cCBA3j8+DG8vLzQtm1brF69+oVrBsLDw9G9e3e8+eab6N+/P4YMGYLGjRsrj+vo6CAhIQEFBQXo0KEDxo8fj88//1xlDBMTExw6dAiOjo4YOnQomjZtinHjxiEvL09ZzgwNDcWYMWPg5+cHT09PmJmZ4a233tLG2yh5ISEhWL16NW7evFnm8aioKMyfPx+tW7fG4cOH8eOPP8La2hoAYG9vj99++w0lJSXo06cPWrRogaCgIMjlcujo8K+9tjVs2BCHDx9Gamoqevfujfbt2yMuLg5xcXF49913tfKc/HmgiuJliImqmWvXrsHJyQmnTp2Cu7u72OGQyPjzQK+LqSAREZGEMREgIiKSME4NEBERSRgrAkRERBLGRICIiEjCmAgQERFJGBMBIiIiCWMiQEREJGFMBIi0YNasWSqbu/j5+ZV5ESptu3btGmQyGU6fPq2153j+tVZEZcRJRGVjIkCS4efnB5lMBplMBn19fTRq1AhTpkxRbuusTV999RXi4uLK1beyvxS9vb0RHBxcKc9FRFUPL0NMktK3b1+sW7cORUVFOHz4MMaPH4/c3FzExMSU6ltUVPTCazuoSy6Xa2QcIiJNY0WAJMXQ0BB2dnZwcHDAqFGjMHr0aGzfvh3A/0rca9euRaNGjWBoaAhBEJCdnY0PPvgANjY2MDc3xxtvvIG//vpLZdyoqCjY2trCzMwM/v7+yM/PVzn+/NSAQqHA/Pnz4ezsDENDQzg6OiovAuXk5AQA8PDwgEwmg7e3t/Jx69atQ9OmTWFkZIQmTZpgxYoVKs/zxx9/wMPDA0ZGRmjXrh1OnTr12u/ZtGnT4OrqChMTEzRq1AgzZswo8/LYq1atgoODA0xMTPDuu++Wuirlq2InInGwIkCSZmxsrPKlduXKFWzduhU//PCD8jLOAwYMgKWlJX7++WfI5XKsWrUKPXv2xKVLl2BpaYmtW7ciIiICy5cvR7du3bBx40YsWbIEjRo1euHzhoeHY/Xq1Vi0aBG6du2K9PR0XLhwAcDTL/MOHTpg3759aN68OQwMDAAAq1evRkREBJYtWwYPDw+cOnUKEyZMgKmpKXx9fZGbm4s333wTb7zxBjZt2oS0tDQEBQW99ntkZmaGuLg42Nvb4+zZs5gwYQLMzMwQFhZW6n3buXMncnJy4O/vj8DAQGzevLlcsRORiAQiifD19RUGDx6svP/7778LVlZWwrBhwwRBEISIiAhBX19fuHv3rrLP/v37BXNzcyE/P19lrMaNGwurVq0SBEEQPD09hY8++kjleMeOHYXWrVuX+dw5OTmCoaGhsHr16jLjTEtLEwAIp06dUml3cHAQvvnmG5W2OXPmCJ6enoIgCMKqVasES0tLITc3V3k8JiamzLH+zcvLSwgKCnrh8edFR0cLbdu2Vd6PiIgQdHV1hZs3byrbdu/eLejo6Ajp6enliv1Fr5mItI8VAZKUXbt2oVatWiguLkZRUREGDx6MpUuXKo83aNAAderUUd4/efIkHj9+DCsrK5Vx8vLykJqaCgBISUnBRx99pHLc09MTBw8eLDOGlJQUFBQUoGfPnuWO+969e7h58yb8/f0xYcIEZXtxcbFy/UFKSgpat24NExMTlThe1/fff4/FixfjypUrePz4MYqLi2Fubq7Sx9HREfXr11d5XoVCgYsXL0JXV/eVsROReJgIkKT06NEDMTEx0NfXh729fanFgKampir3FQoF6tati19//bXUWLVr165QDMbGxmo/RqFQAHhaYu/YsaPKsWdTGIIWrh92/PhxjBgxArNnz0afPn0gl8sRHx+PL7/88qWPk8lkyv+XJ3YiEg8TAZIUU1NTODs7l7t/mzZtkJGRAT09PTRs2LDMPk2bNsXx48cxZswYZdvx48dfOKaLiwuMjY2xf/9+jB8/vtTxZ2sCSkpKlG22traoV68erl69itGjR5c5brNmzbBx40bk5eUpk42XxVEev/32Gxo0aIDp06cr265fv16q340bN3Dnzh3Y29sDAI4dOwYdHR24urqWK3YiEg8TAaKX6NWrFzw9PTFkyBDMnz8fbm5uuHPnDn7++WcMGTIE7dq1Q1BQEHx9fdGuXTt07doVmzdvxvnz51+4WNDIyAjTpk1DWFgYDAwM0KVLF9y7dw/nz5+Hv78/bGxsYGxsjMTERNSvXx9GRkaQy+WYNWsWJk+eDHNzc/Tr1w8FBQVITk5GVlYWQkJCMGrUKEyfPh3+/v7473//i2vXrmHBggXlep337t0rtW+BnZ0dnJ2dcePGDcTHx6N9+/b46aefkJCQUOZr8vX1xYIFC5CTk4PJkydj2LBhsLOzA4BXxk5EIhJ7kQJRZXl+seDzIiIiVBb4PZOTkyNMmjRJsLe3F/T19QUHBwdh9OjRwo0bN5R9Pv/8c8Ha2lqoVauW4OvrK4SFhb1wsaAgCEJJSYkwd+5coUGDBoK+vr7g6OgozJs3T3l89erVgoODg6CjoyN4eXkp2zdv3iy4u7sLBgYGgoWFhdC9e3dh27ZtyuPHjh0TWrduLRgYGAju7u7CDz/8UK7FggBK3SIiIgRBEISpU6cKVlZWQq1atYThw4cLixYtEuRyean3bcWKFYK9vb1gZGQkDB06VMjMzFR5npfFzsWCROKRCYIWJhaJiIioWuCGQkRERBLGRICIiEjCmAgQERFJGBMBIiIiCWMiQEREJGFMBIiIiCSMiQAREZGEMREgIiKSMCYCREREEsZEgIiISMKYCBAREUnY/wFOoeqDo/E/fAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Occluded\", \"Ripe\", \"Unripe\"],\n",
    "            yticklabels=[\"Occluded\", \"Ripe\", \"Unripe\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6738b-aa79-46cd-b4db-11e74f289b6d",
   "metadata": {},
   "source": [
    "- Rows = true labels, columns = predicted labels (classes: 0=Occluded, 1=Ripe, 2=Unripe).\n",
    "\n",
    "    - Class 0 (Occluded): 32 correct, 51 misclassified as Unripe:- detection of occluded strawberries is the weakest point.\n",
    "\n",
    "    - Class 1 (Ripe): 83 of 92 correctly classified:- strong performance.\n",
    "\n",
    "    - Class 2 (Unripe): 479 of 482 correctly classified:- excellent performance.\n",
    "\n",
    "Overall, the model is very strong for Ripe and Unripe classes but struggles more with Occluded images. This behaviour matches the dataset imbalance (Occluded is a minority class) and the visual difficulty of occlusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3211e-1cfb-418f-86df-3a86472c8fab",
   "metadata": {},
   "source": [
    "## 6.3 Model after Class Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d8950-722e-4977-b3d2-eee8fae68626",
   "metadata": {},
   "source": [
    "### 1. Via Class Weights (BEST + EASIEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e73e08a2-a5dc-47d0-b579-fe8f6ec99824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 2.2497911445279866, 1: 2.426126126126126, 2: 0.4665627165627166}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b230dd-8b11-45ea-9f2b-15bde2a76a09",
   "metadata": {},
   "source": [
    "- This chunk calculates **class weights** to handle class imbalance in the training set. Classes 0 and 1 are minority classes, so they are assigned higher weights (≈2.25 and 2.43) while the majority class 2 gets a smaller weight (≈0.47). Applying these weights during training tells the model to **pay more attention to underrepresented classes**, helping reduce bias toward the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a22156ed-8690-4318-9a9f-fb376f5c0718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 770ms/step - accuracy: 0.8312 - loss: 0.4909 - val_accuracy: 0.8741 - val_loss: 0.3844\n",
      "Epoch 2/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 759ms/step - accuracy: 0.8415 - loss: 0.4318 - val_accuracy: 0.8815 - val_loss: 0.3861\n",
      "Epoch 3/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 820ms/step - accuracy: 0.8535 - loss: 0.3643 - val_accuracy: 0.8556 - val_loss: 0.3805\n",
      "Epoch 4/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 841ms/step - accuracy: 0.8506 - loss: 0.3542 - val_accuracy: 0.8778 - val_loss: 0.4041\n",
      "Epoch 5/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 873ms/step - accuracy: 0.8481 - loss: 0.3360 - val_accuracy: 0.8444 - val_loss: 0.4255\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,   \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fd76e28-19fd-4919-947b-39a9099ae0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final training accuracy (balanced training): 0.8481\n",
      "\n",
      "Evaluating on test data...\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 202ms/step - accuracy: 0.8828 - loss: 0.3820\n",
      "\n",
      "Test accuracy: 0.8828\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 201ms/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 45  14  41]\n",
      " [ 10  81   1]\n",
      " [ 12   1 469]]\n"
     ]
    }
   ],
   "source": [
    "# Final training accuracy from last epoch in history\n",
    "final_train_acc = history.history[\"accuracy\"][-1]\n",
    "print(f\"\\nFinal training accuracy (balanced training): {final_train_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Evaluate on ORIGINAL test data\n",
    "# --------------------------------------------------\n",
    "print(\"\\nEvaluating on test data...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Confusion Matrix on test data\n",
    "# --------------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d154780-d8d4-42dc-b400-7c86b246bd51",
   "metadata": {},
   "source": [
    "- The training history shows that the **final training accuracy (balanced)** is 0.8481, and the **test accuracy** is 0.8828. The confusion matrix indicates that minority classes were predicted slightly better than without weighting, though the model still performs best on the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71fb50-a83c-41d5-a5ed-563d98e299f3",
   "metadata": {},
   "source": [
    "### 2. Via Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17020059-bf9f-49d8-acbb-ff03b3a70083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before manual augmentation: {0: 399, 1: 370, 2: 1924}\n",
      "\n",
      "After manual augmentation:\n",
      "{0: 1924, 1: 1924, 2: 1924}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Manual data augmentation for minority classes (0,1)\n",
    "# --------------------------------------------------\n",
    "\n",
    "aug = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Convert to float for augmentation\n",
    "X_train_augmented = []\n",
    "y_train_augmented = []\n",
    "\n",
    "# Count samples per class\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\nBefore manual augmentation:\", dict(zip(unique, counts)))\n",
    "\n",
    "max_samples = max(counts)  # target count = class 2 count (majority class)\n",
    "\n",
    "for class_id in unique:\n",
    "    X_class = X_train[y_train == class_id]\n",
    "    y_class = y_train[y_train == class_id]\n",
    "\n",
    "    # Append original samples\n",
    "    X_train_augmented.append(X_class)\n",
    "    y_train_augmented.append(y_class)\n",
    "\n",
    "    # If minority class → augment\n",
    "    if class_id != 2:  # Only augment class 0 and 1\n",
    "        needed = max_samples - len(X_class)\n",
    "        \n",
    "        # Take random samples from this class\n",
    "        idx = np.random.choice(len(X_class), size=needed, replace=True)\n",
    "        samples_to_augment = X_class[idx]\n",
    "\n",
    "        # Apply augmentation\n",
    "        augmented_images = aug(samples_to_augment, training=True)\n",
    "\n",
    "        X_train_augmented.append(augmented_images.numpy())\n",
    "        y_train_augmented.append(np.full(needed, class_id))\n",
    "\n",
    "# Combine all arrays\n",
    "X_train_balanced = np.concatenate(X_train_augmented, axis=0)\n",
    "y_train_balanced = np.concatenate(y_train_augmented, axis=0)\n",
    "\n",
    "print(\"\\nAfter manual augmentation:\")\n",
    "unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950eba8-d62a-480f-b8c8-7643a44b0577",
   "metadata": {},
   "source": [
    "- Here, minority classes are artificially increased using **data augmentation** (random flips, rotations, brightness, contrast). After augmentation, all classes have the same number of samples (1924 each).\n",
    "\n",
    "- This ensures the model sees a **balanced number of examples for all classes**, which can improve learning for underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a5e4971-3e81-464b-a2bb-0cd7b93ed76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on balanced data...\n",
      "\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 896ms/step - accuracy: 0.6192 - loss: 1.3742 - val_accuracy: 0.9619 - val_loss: 0.1985\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 987ms/step - accuracy: 0.6421 - loss: 0.5911 - val_accuracy: 0.9637 - val_loss: 0.1884\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 966ms/step - accuracy: 0.6521 - loss: 0.5641 - val_accuracy: 0.9602 - val_loss: 0.1652\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 963ms/step - accuracy: 0.6610 - loss: 0.5434 - val_accuracy: 0.9654 - val_loss: 0.2067\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 968ms/step - accuracy: 0.6563 - loss: 0.5343 - val_accuracy: 0.9671 - val_loss: 0.1836\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - accuracy: 0.6746 - loss: 0.5197 - val_accuracy: 0.9567 - val_loss: 0.1804\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1606s\u001b[0m 10s/step - accuracy: 0.6619 - loss: 0.5107 - val_accuracy: 0.9446 - val_loss: 0.1987\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 1s/step - accuracy: 0.6708 - loss: 0.5046 - val_accuracy: 0.9498 - val_loss: 0.2218\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Final training accuracy (balanced training): 0.6708\n",
      "\n",
      "Evaluating on test data...\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - accuracy: 0.8872 - loss: 0.3642\n",
      "\n",
      "Test accuracy: 0.8872\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 272ms/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 37  22  41]\n",
      " [  1  89   2]\n",
      " [  8   2 472]]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Train the model on the BALANCED data\n",
    "# --------------------------------------------------\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training on balanced data...\\n\")\n",
    "history = model.fit(\n",
    "    X_train_balanced,\n",
    "    y_train_balanced,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,   # 10% of BALANCED training data used for validation\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final training accuracy from last epoch in history\n",
    "final_train_acc = history.history[\"accuracy\"][-1]\n",
    "print(f\"\\nFinal training accuracy (balanced training): {final_train_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Evaluate on ORIGINAL test data\n",
    "# --------------------------------------------------\n",
    "print(\"\\nEvaluating on test data...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Confusion Matrix on test data\n",
    "# --------------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6628a-a4c6-458f-ac13-9534c38f3216",
   "metadata": {},
   "source": [
    "- However, the final training accuracy dropped to 0.6708, while the test accuracy is 0.8872. This occurs because augmentation introduces **more variability in minority classes**, making the training task harder. The confusion matrix shows better representation of minority classes, but the overall training accuracy suffers compared to unbalanced training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a19e4-cf29-45b6-af47-deb0361fdb27",
   "metadata": {},
   "source": [
    "## 7. Build MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7796d27-da09-4e8a-a099-8d2d0d210314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Preprocessing Complete.\n",
      "Training image shape: (2693, 224, 224, 3)\n",
      "\n",
      "Class Weights: {0: 2.2497911445279866, 1: 2.426126126126126, 2: 0.4665627165627166}\n",
      "\n",
      "--- Starting Training: MobileNetV2 ---\n",
      "Epoch 1/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.7536 - loss: 0.6697 - val_accuracy: 0.8441 - val_loss: 0.4271\n",
      "Epoch 2/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.8410 - loss: 0.4533 - val_accuracy: 0.8738 - val_loss: 0.3913\n",
      "Epoch 3/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.8589 - loss: 0.4006 - val_accuracy: 0.8564 - val_loss: 0.3839\n",
      "Epoch 4/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.8519 - loss: 0.3966 - val_accuracy: 0.8441 - val_loss: 0.4303\n",
      "Epoch 5/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.8777 - loss: 0.3468 - val_accuracy: 0.9010 - val_loss: 0.3185\n",
      "Epoch 6/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 997ms/step - accuracy: 0.8982 - loss: 0.3105 - val_accuracy: 0.8069 - val_loss: 0.4392\n",
      "Epoch 7/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 985ms/step - accuracy: 0.8812 - loss: 0.3054 - val_accuracy: 0.8391 - val_loss: 0.4041\n",
      "Epoch 8/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.8956 - loss: 0.3016 - val_accuracy: 0.8094 - val_loss: 0.4517\n",
      "Epoch 9/30\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.9030 - loss: 0.2553 - val_accuracy: 0.8787 - val_loss: 0.3611\n",
      "\n",
      "Training completed in 773.10 seconds.\n",
      "Final Training Accuracy: 90.30%\n",
      "Test Accuracy: 87.83%\n",
      "\n",
      "==============================\n",
      "Final Model Test Accuracy: 87.83%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_FILE = 'strawberries.npz'\n",
    "IMAGE_SIZE = (224, 224)  # MobileNetV2 input size\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "\n",
    "def build_mobilenet_v2():\n",
    "    \"\"\"Builds a fine-tuned MobileNetV2 model.\"\"\"\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(model_builder, X_train_proc, y_train_cat, X_test_proc, y_test_cat, class_weights_dict):\n",
    "    \"\"\"Compiles, trains, and evaluates MobileNetV2.\"\"\"\n",
    "    print(\"\\n--- Starting Training: MobileNetV2 ---\")\n",
    "\n",
    "    model = model_builder()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train_cat,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stop],\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    final_train_accuracy = history.history['accuracy'][-1]\n",
    "    print(f\"Final Training Accuracy: {final_train_accuracy * 100:.2f}%\")\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_proc, y_test_cat, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "# --- Main script execution ---\n",
    "\n",
    "# === Load dataset ===\n",
    "data = np.load(DATA_FILE)\n",
    "X = data[\"x\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "# Remap labels {1,2,3} → {0,1,2}\n",
    "y = y - 1\n",
    "\n",
    "# === Train/Test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Resize & normalize ===\n",
    "X_train_proc = np.array([tf.image.resize(img, IMAGE_SIZE).numpy() for img in X_train])\n",
    "X_test_proc = np.array([tf.image.resize(img, IMAGE_SIZE).numpy() for img in X_test])\n",
    "\n",
    "# If grayscale → repeat to make 3 channels\n",
    "if X_train_proc.shape[-1] == 1:\n",
    "    X_train_proc = np.repeat(X_train_proc, 3, axis=-1)\n",
    "    X_test_proc = np.repeat(X_test_proc, 3, axis=-1)\n",
    "\n",
    "X_train_proc = X_train_proc.astype(\"float32\") / 255.0\n",
    "X_test_proc = X_test_proc.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_cat = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "print(\"\\nData Preprocessing Complete.\")\n",
    "print(f\"Training image shape: {X_train_proc.shape}\")\n",
    "\n",
    "# === Compute class weights ===\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n",
    "print(f\"\\nClass Weights: {class_weights_dict}\")\n",
    "\n",
    "# === Train MobileNetV2 only ===\n",
    "test_accuracy = train_and_evaluate(\n",
    "    build_mobilenet_v2,\n",
    "    X_train_proc, y_train_cat,\n",
    "    X_test_proc, y_test_cat,\n",
    "    class_weights_dict\n",
    ")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"Final Model Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e0872-ccc2-47d4-9646-c6165e70dc87",
   "metadata": {},
   "source": [
    "- MobileNetV2, a pre-trained CNN on ImageNet, is used to leverage **transfer learning**. Only the top layers are trained on this dataset, reducing overfitting and training time. Class weights are applied during training to address imbalance.\n",
    "\n",
    "- The model achieves a **final training accuracy of 90.30%** and a **test accuracy of 87.83%**. This is slightly lower than your custom CNN but shows strong generalization with **fewer trainable parameters** and faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
